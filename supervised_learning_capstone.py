# -*- coding: utf-8 -*-
"""supervised_learning_capstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hI8P53-9zlIyBYVOZ1PZ4C9UzOdgApRk
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy
from pprint import pprint
import math
from sklearn.model_selection import train_test_split
from sklearn import ensemble
import seaborn as sns
from sklearn.metrics import confusion_matrix, precision_score, recall_score
import itertools
import math
from scipy.stats import boxcox
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score, classification_report
from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score
from sklearn.model_selection import RandomizedSearchCV

# %matplotlib inline

"""This dataset is concered with user data for a retail website detailing user habits, seasonal data, and page visit rates. The final column is a True False column details a True or False value in regards to whether the customer generated revenue or not. Below we will explore the columns and explore the head of DataFrame to get an idea of the features."""

data = pd.read_csv('https://raw.githubusercontent.com/johnadrian45/John-s-Projects/master/online_shoppers_intention.csv')
data['Revenue_Integer'] = data.Revenue.map({True:1, False:0})
data.Revenue_Integer.value_counts()

#Separate the features into the repsectice data types
float_cols = list(data.select_dtypes(include = 'float64').columns)
object_cols = list(data.select_dtypes(include= 'object').columns)
bool_cols = list(data.select_dtypes(include= 'bool').columns)
int_cols = list(data.select_dtypes(include= 'int64').columns)

# Look for missing values
data.isna().sum()

"""Above we see that there are some missing values in first columns, particularly the float-type columns, specifically the 'Administrative', 'ProductRelated' and 'Informational' columns, along with thier durations are missing. These columns represent the number of pages relevant to that category visited by the user as well as the time spent on each of these pages. There could be various reasons for these missing values, from not collected data, to human error, or a true '0' for those values.
  In a production enviornment efforts should and would be made to explore the origin of that missing data, and the data would be either dropped or filled accordingly. However, in the vacuum of this excersize we will simply assume that the missing values imply that the user simply did not visit/did not spend time on these pages.
  It should be noted that there were only 14 missing values, in 5 of the features, which is a small percentage of all the available data, however it should sitll be addressed. 
  Below we will fill the missing values with zeroes, however we will add additional columns to the DataFrame tracking which columns were altered.
"""

# Method that will fill null values in data frame and create new column
# used to track changed for reference
def fill_columns(df, features):
  for col in features:
    df['{}_fills'.format(col)] = np.where(df[col].isnull(), 1, 0)
  df.loc[:,features] = data.loc[:,features].fillna(0)

fill_columns(data, float_cols)
print(data.isnull().sum())

"""Here we will do some feature engineering, which will add some depth to our model. Some of the most interesting columns are the columns that had missing features, 'Administrative', 'Informational', and 'ProductRelated', which specify the number of that type of page visited by the user. We also have data about how much time the user spent, on average, on these pages. From this data we can find the total number of visits for each type of page, the total amount of time spent on each page, and the proportion of each type of page to the total number of pages visited.
  Another interesting metric is 'page value' which a value that takes how many pages the user visited before a transaction, and assigns an average value. We can use this metric to create weighted values for the 3 different types of pages.
  Afterwards we fill these new columns in the event that they have missing values.
"""

data['total_duration'] = data.Administrative_Duration + data.Informational_Duration + data.ProductRelated_Duration
data['total_visits'] = data.Administrative + data.Informational + data.ProductRelated
data['admin_ratio'] = data.Administrative/data.total_visits
data['info_ratio'] = data.Informational/data.total_visits
data['product_ratio'] = data.ProductRelated/data.total_visits
data['admin_value'] = data.admin_ratio * data.PageValues
data['info_value'] = data.info_ratio * data.PageValues
data['product_value'] = data.product_ratio * data.PageValues
data['admin_time_ratio'] = data.Administrative_Duration/data.total_duration

#In this case you can replace NaN with zero
data['admin_time_per_page'] = data.Administrative_Duration//data.Administrative
data['info_time_ratio']  = data.Informational_Duration/data.total_duration
data['info_time_per_page'] = data.Informational_Duration//data.Informational
data['product_time_ratio'] = data.ProductRelated_Duration/data.total_duration
data['product_time_per_page'] = data.ProductRelated_Duration//data.ProductRelated
new_features = ['total_duration', 'total_visits', 'admin_ratio', 'info_ratio', 'product_ratio', 'admin_value', 'info_value', 'product_value',
                'admin_time_ratio', 'info_time_ratio', 'product_time_ratio']
float_cols = float_cols + new_features
fill_columns(data, new_features)
print(data.isnull().sum())

plt.figure(figsize=(15,7))
plt.subplot(1,2,1)
sns.barplot(data.groupby('Revenue')['total_duration'].mean().index, data.groupby('Revenue')['total_duration'].mean())
plt.title('Revenue Generation with respect to Time Spent Browsing')
plt.subplot(1,2,2)
sns.barplot(data.groupby('Revenue')['total_visits'].mean().index, data.groupby('Revenue')['total_visits'].mean())
plt.title('Revenue Generation with respect to Total Pages Visisted')
plt.tight_layout()
plt.show()

# Methods for finding nearest square number. Used for better visualization
def is_square(num):
  if (math.sqrt(num) - math.floor(math.sqrt(num))) == 0:
    return True
  else:
    return False

def find_square(num):
  while is_square(num) == False:
    num = num + 1
  return math.sqrt(num)

# Method for visualization of distribution of features
def create_histograms(df, features):
  subplot = find_square(len(features))
  plt.figure(figsize=(30,30))
  for index, col in enumerate(features):
    plt.subplot(subplot, subplot, index + 1)
    plt.hist(df[col])
    plt.title('Distribution for: {}'.format(col))
  plt.show()



# Method for visualiztion of relationship between numerical featuers and
# binary target, prediction
def plot_floats_vs_target(df, features, target):
  plt.figure(figsize=(30,30))
  subplot = find_square(len(features))
  for index, col in enumerate(features):
    plt.subplot(subplot, subplot, index + 1)
    #use median instead
    sns.barplot(df.groupby(target)[col].median().index, df.groupby(target)[col].median())
    plt.title('Revenue with respect to {} median'.format(col))
  plt.show()

"""Below we have the distributions of our continous variables. Interestingly, none of them are normally distributed and we might benefit from normally distributed variables for our model. Firstly we will attempt to use these models unaltered, then we will use the normally distributed versions."""

create_histograms(data, float_cols)

create_histograms(data, object_cols)

"""Here we have one of the more important visualizations of the data. Bar charts were created with True False values in regards to revenue, with respect to the averages of the continous numerical data. 
  Basically we are visualizing the average number for each datapoint that produces either a True or False value in regards to revenue. For example, the Admisnistrative charges chart shows us that an average of 2.5 visits to an administrative webpage leads to a False value, while an average of 3.25 visits to an administrative page leads to a True value.
  The insights that we can gain from this data is that in regards to the number of pages visited, and the amount of time spent on them, the more pages and the more time spent, the more likely it is true that revenue will be generated.
  Furthemore, we see that bounce rates and exit rates have a negative affect on Revenue, as the higher the bounce rate or the exit rate, the more likely it is that customer will not generate Revenue.
  Special Days, or holidays do not have a significant affect on revenue.
"""

plot_floats_vs_target(data, float_cols, 'Revenue')

"""Below are some tools that we will use to visualize and interprept the effectiveness of our model. We wil print a classification report and a normalized confusion matrix to examine model results."""

def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True):
    """
    Citiation
    ---------
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html

    """
    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('Predicted label')
    plt.xlabel('True label')
    plt.show()

"""After visualizing and interpreting our data, we now move on the creating dummy variables for the object data types to be added to our model."""

data = pd.concat([data, pd.get_dummies(data.Month, prefix='Month', drop_first= True)], axis=1)
data = pd.concat([data, pd.get_dummies(data.VisitorType, prefix='VisitorType', drop_first= True)], axis = 1)
dummy_cols = list(pd.get_dummies(data.Month, prefix='Month', drop_first= True).columns)
dummy_cols = dummy_cols + list(pd.get_dummies(data.VisitorType, prefix= 'VisitorType', drop_first= True).columns)

"""The data is now split into our predictive features and our target variables, and test, and train samples of the data are selected as well."""

features = float_cols + dummy_cols + new_features
X = data[features]
Y = data.Revenue_Integer
x_train, x_test, y_train, y_test = train_test_split(X,Y)

"""The model selected is a Random Forest Classifier. This method was chosen due to is powerful predictive precision and accuracy. In order to avoid overfitting and to find the ideal parameters for the model we will implement a random search algorithm to find the optimized hyperparameters. 
  This algorithm also uses k-fold cross validation to avoid overfitting. Random Forests are particularly suceptible to overfitting and we do not have much visibility into how it is actually dividing the features. This k-fold cross validation helps us to mitigate overfitting.
"""

# add class weights
rfc = ensemble.RandomForestClassifier()
random_grid = {'n_estimators': [int(x) for x in  np.arange(50,500)], # change to 50 to 500 a huge number of trees can overfit
              'max_features': ['auto', 'sqrt'],
              'max_depth' : [int(x) for x in np.arange(2, 50)], # bring it back to prevent overfitting 2 - 50
              'min_samples_split' : [int(x) for x in np.arange(2,100)], # these can be a range 
              'min_samples_leaf': [int(x) for x in np.arange(1,4)], # these can be a range
              'bootstrap': [True, False],
              'class_weight': [{True: 10, False: 1}]}
rfc_random = RandomizedSearchCV(estimator= rfc, 
                                param_distributions= random_grid,
                                n_iter = 100, 
                                cv = 2, 
                                verbose = 2, 
                                random_state = 42,
                                n_jobs = -1)
rfc_random.fit(x_train, y_train)
best_params = rfc_random.best_params_
optimized_rfc = ensemble.RandomForestClassifier(n_estimators= best_params['n_estimators'],
                                              max_depth = best_params['max_depth'],
                                              min_samples_leaf = best_params['min_samples_leaf'],
                                              min_samples_split = best_params['min_samples_split'],
                                              bootstrap = best_params['bootstrap'],
                                              max_features = best_params['max_features'],
                                              class_weight = best_params['class_weight']
                                              )

"""Here we implement our algorithm to find an optimally parameterized model.

After finding a model that is optimally parameterized we will fit this model to our training data and evaluate its performance using classification reports and confusion matrices.
  Here we see that the model was quite effective, achieving a precision on False outcomes of 96% and 94% for True outcomes. Our recalls have similar results with percentages for True and False both above 90%. 
  Analyzing the normalized confusion matrix, we see that we have achieved almost 99% accuracy in our predictions.
  According to the classification report we have built an extremely effective Random Forest Classifier model. However, we are in the dark as to how the model actually arrived at these values. Not only is the Random Forest intrinsically a black box, but we also do not have clarity as to the ideal hyperparameters for this model, as we used a randomized search to find the ideal parameters. While we have powerful predictive power, we are trading visibility into the mechanics of the model.
"""

optimized_rfc.fit(x_train, y_train)
train_preds = optimized_rfc.predict(x_train)
print(classification_report(y_train, train_preds))
train_confusion = confusion_matrix(y_train, train_preds)
plot_confusion_matrix(train_confusion, ['True', 'False'], title= 'Train Set Confusion Matrix', normalize= True)

test_preds = optimized_rfc.predict(x_test)
test_confusion = confusion_matrix(y_test, test_preds)
print(classification_report(y_test, test_preds))
plot_confusion_matrix(test_confusion, ['True', 'False'], title= 'Test Set Confusion Matrix', normalize= True)

"""Below we will replicate our efforts with a gradient boosted model."""

gbm = ensemble.GradientBoostingClassifier()
gbm_random_grid = {'learning_rate': [0.15, 0.01, 0.05, 0.01, 0.005, 0.001],
                   'loss' : ['deviance', 'exponential'],
                   'n_estimators': [int(x) for x in np.arange(1,10)],
                   'max_depth': [2,3,4,5,6,7],
                   'criterion': ['friedman_mse', 'mse', 'mae'],
                   'min_samples_split': [int(x) for x in np.arange(2,20)],
                   'min_samples_leaf': [int(x) for x in np.arange(2,20)],
                   'max_features': ['auto', 'sqrt', 'log2']}
gbm_random = RandomizedSearchCV(estimator= gbm, 
                                param_distributions= gbm_random_grid,
                                n_iter = 10, 
                                cv = 3, 
                                verbose = 2, 
                                random_state = 42,
                                n_jobs = -1)
gbm_random.fit(x_train, y_train)
best_params = gbm_random.best_params_
print(best_params)

optimized_gbm = ensemble.GradientBoostingClassifier(loss = best_params['loss'],
                                          criterion = best_params['criterion'],
                                          n_estimators= best_params['n_estimators'],
                                          min_samples_split = best_params['min_samples_split'],
                                          min_samples_leaf = best_params['min_samples_leaf'],
                                          max_features = best_params['max_features'],
                                          max_depth = best_params['max_depth'],
                                          learning_rate = best_params['learning_rate'])
optimized_gbm.fit(x_train, y_train)
gbm_train_preds = optimized_gbm.predict(x_train)
print(classification_report(y_train, gbm_train_preds))
train_confusion = confusion_matrix(y_train, gbm_train_preds)
plot_confusion_matrix(train_confusion, ['True', 'False'], title= 'Train Set Confusion Matrix', normalize= True)

gbm_test_preds = optimized_gbm.predict(x_test)
print(classification_report(y_test, gbm_test_preds))
test_confusion = confusion_matrix(y_test, gbm_test_preds)
plot_confusion_matrix(test_confusion, ['True', 'False'], title = 'Test Set Confusion Matrix', normalize= True)

"""Taking a look at our GBM model, we see that it has achieved similar results as our optimized random forest. Either model could work well, however the speed of the GBM gives it a slight advantage.

All in all this is a successful model that achieves good margins of accuracy and precision. 

This question that this research answers is: 'How can we predict whether a customer will create revenue or not based on thier browsing data?' This model answers that questions by taking the features inherent to the data and adding features from the EDA process to accurately predict revenue generating customers.

I chose the Random Forest Classifier because of its incredible predictive power, and addressed the issue of overfitting by using a random search algorithm along with k-fold validation to create an optimized classifier. Another model that would work well in this instance would be a K-Nearest Neighbors algorithm, however due to the high dimensionality of the dataset, I felt that a KNearest Neighbors algorithm would be sugglish and difficult to fine tune. On the other hand, the Random Forest couples with a random search for optimized hyperparameters is quite the black box and there is very little transparency as to how such a high accuracy score was achieved. 

This type of model is highly useful to many different business. In today's digital world online retail is growing giant, and in order to stay competitive business need to adapt to the online market and they need to have a way to measure successful revenue generation. This model takes a user's browsing data and shows which users will produce revenue. The insights that can be taken from this is what type of user behaviors are most likely to produce revenue, and further research can be done to find a way to influence these behaviors to maximize revenue.
"""